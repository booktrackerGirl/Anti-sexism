{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 13:28:25.080637: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-22 13:28:25.081634: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-22 13:28:25.084296: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-22 13:28:25.091899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-22 13:28:25.104069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-22 13:28:25.107738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-22 13:28:25.117070: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-22 13:28:25.725317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcardinal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeansSampler\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcardinal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RankedBatchSampler\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m  \u001b[38;5;66;03m# Registers the ops.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow_text/__init__.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybinds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tflite_registrar\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow_text/python/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m _allowed_symbols \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m ]\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow_text/python/keras/layers/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtodense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[39;00m\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow_text/python/keras/layers/todense.py:28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mToDense\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241m.\u001b[39mLayer):  \u001b[38;5;66;03m# pylint: disable=g-classes-have-attributes\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Layer that makes padding and masking a Composite Tensors effortless.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m  The layer takes a RaggedTensor or a SparseTensor and converts it to a uniform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    a ragged input or the same dense shape, in case of a sparse input.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:181\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(types\u001b[38;5;241m.\u001b[39mModuleType, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_initialized:\n\u001b[0;32m--> 181\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_keras_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\n\u001b[1;32m    186\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m   ):\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:147\u001b[0m, in \u001b[0;36mKerasLazyLoader._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    150\u001b[0m       \u001b[38;5;66;03m# This is the Keras 3.x case.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m       keras_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/engine/functional.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/engine/training.py:45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hdf5_format\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/saving/legacy/save.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n",
      "File \u001b[0;32m~/Anti-sexism/antisexism_venv/lib/python3.10/site-packages/keras/saving/legacy/saved_model/load_context.py:68\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import emoji\n",
    "import contractions\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.discard('not')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from cardinal.random import RandomSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cardinal.uncertainty import ConfidenceSampler\n",
    "from cardinal.utils import ActiveLearningSplitter\n",
    "from cardinal.clustering import KMeansSampler\n",
    "from cardinal.batch import RankedBatchSampler\n",
    "\n",
    "import tensorflow_text as text  # Registers the ops.\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes in considerations:\n",
    "- https://github.com/ritun16/Machine_Learning_short_projecct/blob/master/Active_Learning/Active_Learning_Data_Annotation.ipynb\n",
    "- https://github.com/zhao-shuyang/active_learning\n",
    "- https://dataiku-research.github.io/cardinal/auto_examples/plot_random_vs_confidence.html\n",
    "- https://dataiku-research.github.io/cardinal/auto_examples/plot_hard_to_classify.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 1797\\n:Number of Attributes: 64\\n:Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n:Missing Attribute Values: None\\n:Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n:Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n  Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n  Graduate Studies in Science and Engineering, Bogazici University.\\n- E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n- Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n  Linear dimensionalityreduction using relevance weighted LDA. School of\\n  Electrical and Electronic Engineering Nanyang Technological University.\\n  2005.\\n- Claudio Gentile. A New Approximate Maximal Margin Classification\\n  Algorithm. NIPS. 2000.\\n\\n|details-end|\\n\"}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antisexm24_uk-0</td>\n",
       "      <td>The rise of Suella Braverman is one of the gen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antisexm24_uk-1</td>\n",
       "      <td>General Election please! 81,000 Tory members v...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antisexm24_uk-2</td>\n",
       "      <td>Appointing Suella Braverman as Home secretary ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antisexm24_uk-3</td>\n",
       "      <td>Disgusted to hear Suella Braverman say there i...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antisexm24_uk-4</td>\n",
       "      <td>Just bumped into Suella Braverman who said to ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115186</th>\n",
       "      <td>antisexm24_uk-115186</td>\n",
       "      <td>@MichelleDewbs @trussliz She was clear, concis...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115187</th>\n",
       "      <td>antisexm24_uk-115187</td>\n",
       "      <td>@SamCoatesSky @RupaHuq @SuellaBraverman Extrao...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115188</th>\n",
       "      <td>antisexm24_uk-115188</td>\n",
       "      <td>Every time Liz Truss talks about her mandate o...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115189</th>\n",
       "      <td>antisexm24_uk-115189</td>\n",
       "      <td>@itaisher I think I just mean the \"de facto\" t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115190</th>\n",
       "      <td>antisexm24_uk-115190</td>\n",
       "      <td>@HappyUlster @jameshhowe @13MoonWillow @benhab...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115191 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueID  \\\n",
       "0            antisexm24_uk-0   \n",
       "1            antisexm24_uk-1   \n",
       "2            antisexm24_uk-2   \n",
       "3            antisexm24_uk-3   \n",
       "4            antisexm24_uk-4   \n",
       "...                      ...   \n",
       "115186  antisexm24_uk-115186   \n",
       "115187  antisexm24_uk-115187   \n",
       "115188  antisexm24_uk-115188   \n",
       "115189  antisexm24_uk-115189   \n",
       "115190  antisexm24_uk-115190   \n",
       "\n",
       "                                                     text label  \n",
       "0       The rise of Suella Braverman is one of the gen...        \n",
       "1       General Election please! 81,000 Tory members v...        \n",
       "2       Appointing Suella Braverman as Home secretary ...        \n",
       "3       Disgusted to hear Suella Braverman say there i...        \n",
       "4       Just bumped into Suella Braverman who said to ...        \n",
       "...                                                   ...   ...  \n",
       "115186  @MichelleDewbs @trussliz She was clear, concis...        \n",
       "115187  @SamCoatesSky @RupaHuq @SuellaBraverman Extrao...        \n",
       "115188  Every time Liz Truss talks about her mandate o...        \n",
       "115189  @itaisher I think I just mean the \"de facto\" t...        \n",
       "115190  @HappyUlster @jameshhowe @13MoonWillow @benhab...        \n",
       "\n",
       "[115191 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv('../data/orig/original_full_dataset.csv')\n",
    "full_data = full_data[['uniqueID', 'text']]\n",
    "full_data['label'] = ''\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antisexm24_uk_remaining-0</td>\n",
       "      <td>She has NOTHING on Braverman. Nothing. \\n\\nSue...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antisexm24_uk_remaining-1</td>\n",
       "      <td>@10DowningStreet @SuellaBraverman @ukhomeoffic...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antisexm24_uk_remaining-2</td>\n",
       "      <td>@_TheGMan @benandjerrysUK @SuellaBraverman Goo...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antisexm24_uk_remaining-3</td>\n",
       "      <td>@Jerry_grey2002 @drmikechao @TsarKastik But I ...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antisexm24_uk_remaining-4</td>\n",
       "      <td>@10DowningStreet @SuellaBraverman @ukhomeoffic...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>antisexm24_uk_remaining-95</td>\n",
       "      <td>Boris Johnson visited Gigg Lane stadium today....</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>antisexm24_uk_remaining-96</td>\n",
       "      <td>@EmilyThornberry @PETAUK This at labour won’t ...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>antisexm24_uk_remaining-97</td>\n",
       "      <td>Queen 👑 @AngelaRayner https://t.co/7HDiEHy3sJ</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>antisexm24_uk_remaining-98</td>\n",
       "      <td>UK Prime Minister Boris Johnson lashed out at ...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>antisexm24_uk_remaining-99</td>\n",
       "      <td>@ladyhaja @AngelaRayner Most debating I ever s...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      uniqueID  \\\n",
       "0    antisexm24_uk_remaining-0   \n",
       "1    antisexm24_uk_remaining-1   \n",
       "2    antisexm24_uk_remaining-2   \n",
       "3    antisexm24_uk_remaining-3   \n",
       "4    antisexm24_uk_remaining-4   \n",
       "..                         ...   \n",
       "95  antisexm24_uk_remaining-95   \n",
       "96  antisexm24_uk_remaining-96   \n",
       "97  antisexm24_uk_remaining-97   \n",
       "98  antisexm24_uk_remaining-98   \n",
       "99  antisexm24_uk_remaining-99   \n",
       "\n",
       "                                                 text    label  \n",
       "0   She has NOTHING on Braverman. Nothing. \\n\\nSue...   sexism  \n",
       "1   @10DowningStreet @SuellaBraverman @ukhomeoffic...  neither  \n",
       "2   @_TheGMan @benandjerrysUK @SuellaBraverman Goo...  neither  \n",
       "3   @Jerry_grey2002 @drmikechao @TsarKastik But I ...  neither  \n",
       "4   @10DowningStreet @SuellaBraverman @ukhomeoffic...  neither  \n",
       "..                                                ...      ...  \n",
       "95  Boris Johnson visited Gigg Lane stadium today....  neither  \n",
       "96  @EmilyThornberry @PETAUK This at labour won’t ...  neither  \n",
       "97      Queen 👑 @AngelaRayner https://t.co/7HDiEHy3sJ  neither  \n",
       "98  UK Prime Minister Boris Johnson lashed out at ...  neither  \n",
       "99  @ladyhaja @AngelaRayner Most debating I ever s...  neither  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pilot = pd.read_csv('data/pilot/Annotation_Chico 1.csv', encoding='unicode_escape')\n",
    "\n",
    "#temp = pd.read_csv('data/pilot/Annotations_Aditi 1.csv')\n",
    "#pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "pilot = pd.read_csv('../data/pilot/Annotations_Aditi 1.csv')\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Aditi 2.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Aditi 3.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Aditi 4.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Susan 1.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Susan 2.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "temp = pd.read_csv('../data/pilot/Annotations_Susan 3.csv')\n",
    "pilot = pd.concat([pilot[['text', 'label']], temp[['text', 'label']]], axis = 0)\n",
    "\n",
    "pilot['uniqueID'] = pilot.index\n",
    "pilot['uniqueID'] = 'antisexm24_uk_remaining-' + pilot['uniqueID'].astype(str)\n",
    "pilot.insert(0, 'uniqueID', pilot.pop('uniqueID'))  ## repositioning it to the front\n",
    "\n",
    "pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antisexm24_uk_remaining-0</td>\n",
       "      <td>She has NOTHING on Braverman. Nothing. \\n\\nSue...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antisexm24_uk_remaining-1</td>\n",
       "      <td>@10DowningStreet @SuellaBraverman @ukhomeoffic...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antisexm24_uk_remaining-2</td>\n",
       "      <td>@_TheGMan @benandjerrysUK @SuellaBraverman Goo...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antisexm24_uk_remaining-3</td>\n",
       "      <td>@Jerry_grey2002 @drmikechao @TsarKastik But I ...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antisexm24_uk_remaining-4</td>\n",
       "      <td>@10DowningStreet @SuellaBraverman @ukhomeoffic...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115186</th>\n",
       "      <td>antisexm24_uk-115186</td>\n",
       "      <td>@MichelleDewbs @trussliz She was clear, concis...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115187</th>\n",
       "      <td>antisexm24_uk-115187</td>\n",
       "      <td>@SamCoatesSky @RupaHuq @SuellaBraverman Extrao...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115188</th>\n",
       "      <td>antisexm24_uk-115188</td>\n",
       "      <td>Every time Liz Truss talks about her mandate o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115189</th>\n",
       "      <td>antisexm24_uk-115189</td>\n",
       "      <td>@itaisher I think I just mean the \"de facto\" t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115190</th>\n",
       "      <td>antisexm24_uk-115190</td>\n",
       "      <td>@HappyUlster @jameshhowe @13MoonWillow @benhab...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115980 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         uniqueID  \\\n",
       "0       antisexm24_uk_remaining-0   \n",
       "1       antisexm24_uk_remaining-1   \n",
       "2       antisexm24_uk_remaining-2   \n",
       "3       antisexm24_uk_remaining-3   \n",
       "4       antisexm24_uk_remaining-4   \n",
       "...                           ...   \n",
       "115186       antisexm24_uk-115186   \n",
       "115187       antisexm24_uk-115187   \n",
       "115188       antisexm24_uk-115188   \n",
       "115189       antisexm24_uk-115189   \n",
       "115190       antisexm24_uk-115190   \n",
       "\n",
       "                                                     text    label  \n",
       "0       She has NOTHING on Braverman. Nothing. \\n\\nSue...   sexism  \n",
       "1       @10DowningStreet @SuellaBraverman @ukhomeoffic...  neither  \n",
       "2       @_TheGMan @benandjerrysUK @SuellaBraverman Goo...  neither  \n",
       "3       @Jerry_grey2002 @drmikechao @TsarKastik But I ...  neither  \n",
       "4       @10DowningStreet @SuellaBraverman @ukhomeoffic...  neither  \n",
       "...                                                   ...      ...  \n",
       "115186  @MichelleDewbs @trussliz She was clear, concis...      NaN  \n",
       "115187  @SamCoatesSky @RupaHuq @SuellaBraverman Extrao...      NaN  \n",
       "115188  Every time Liz Truss talks about her mandate o...      NaN  \n",
       "115189  @itaisher I think I just mean the \"de facto\" t...      NaN  \n",
       "115190  @HappyUlster @jameshhowe @13MoonWillow @benhab...      NaN  \n",
       "\n",
       "[115980 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([pilot, full_data], axis=0)\n",
    "data = data.drop_duplicates(subset='text')\n",
    "data['label'] = data[['label']].replace(r'^\\s*$', np.nan, regex=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antisexm24_uk-0</td>\n",
       "      <td>The rise of Suella Braverman is one of the gen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antisexm24_uk-1</td>\n",
       "      <td>General Election please! 81,000 Tory members v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antisexm24_uk-2</td>\n",
       "      <td>Appointing Suella Braverman as Home secretary ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antisexm24_uk-3</td>\n",
       "      <td>Disgusted to hear Suella Braverman say there i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antisexm24_uk-4</td>\n",
       "      <td>Just bumped into Suella Braverman who said to ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115186</th>\n",
       "      <td>antisexm24_uk-115186</td>\n",
       "      <td>@MichelleDewbs @trussliz She was clear, concis...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115187</th>\n",
       "      <td>antisexm24_uk-115187</td>\n",
       "      <td>@SamCoatesSky @RupaHuq @SuellaBraverman Extrao...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115188</th>\n",
       "      <td>antisexm24_uk-115188</td>\n",
       "      <td>Every time Liz Truss talks about her mandate o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115189</th>\n",
       "      <td>antisexm24_uk-115189</td>\n",
       "      <td>@itaisher I think I just mean the \"de facto\" t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115190</th>\n",
       "      <td>antisexm24_uk-115190</td>\n",
       "      <td>@HappyUlster @jameshhowe @13MoonWillow @benhab...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueID  \\\n",
       "0            antisexm24_uk-0   \n",
       "1            antisexm24_uk-1   \n",
       "2            antisexm24_uk-2   \n",
       "3            antisexm24_uk-3   \n",
       "4            antisexm24_uk-4   \n",
       "...                      ...   \n",
       "115186  antisexm24_uk-115186   \n",
       "115187  antisexm24_uk-115187   \n",
       "115188  antisexm24_uk-115188   \n",
       "115189  antisexm24_uk-115189   \n",
       "115190  antisexm24_uk-115190   \n",
       "\n",
       "                                                     text label  \n",
       "0       The rise of Suella Braverman is one of the gen...   NaN  \n",
       "1       General Election please! 81,000 Tory members v...   NaN  \n",
       "2       Appointing Suella Braverman as Home secretary ...   NaN  \n",
       "3       Disgusted to hear Suella Braverman say there i...   NaN  \n",
       "4       Just bumped into Suella Braverman who said to ...   NaN  \n",
       "...                                                   ...   ...  \n",
       "115186  @MichelleDewbs @trussliz She was clear, concis...   NaN  \n",
       "115187  @SamCoatesSky @RupaHuq @SuellaBraverman Extrao...   NaN  \n",
       "115188  Every time Liz Truss talks about her mandate o...   NaN  \n",
       "115189  @itaisher I think I just mean the \"de facto\" t...   NaN  \n",
       "115190  @HappyUlster @jameshhowe @13MoonWillow @benhab...   NaN  \n",
       "\n",
       "[115180 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = data[data['label'].notna()]\n",
    "unlabeled_data = data[data['label'].isna()]\n",
    "\n",
    "unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238       if the Daily Mail thinks its outrageous that K...\n",
       "239       BREAKING: As foreign-born criminal Boris Johns...\n",
       "240       As I said recently, Suella Braverman is absolu...\n",
       "241       Liz Truss might want to give it a few days bef...\n",
       "242       I admit I have still not come to terms with th...\n",
       "                                ...                        \n",
       "115186    @MichelleDewbs @trussliz She was clear, concis...\n",
       "115187    @SamCoatesSky @RupaHuq @SuellaBraverman Extrao...\n",
       "115188    Every time Liz Truss talks about her mandate o...\n",
       "115189    @itaisher I think I just mean the \"de facto\" t...\n",
       "115190    @HappyUlster @jameshhowe @13MoonWillow @benhab...\n",
       "Name: text, Length: 114942, dtype: object"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labelled = labeled_data['text']\n",
    "y_labelled = labeled_data['label']\n",
    "\n",
    "X_unlabelled = unlabeled_data['text']\n",
    "X_unlabelled = X_unlabelled[238:] # 1st batch\n",
    "X_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usual_preprocess(text):\n",
    "\n",
    "    sent = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_result = word\n",
    "        if word.startswith(\"http\"):\n",
    "            word_result = '[URL]'\n",
    "        else:\n",
    "            pass\n",
    "        sent.append(word_result)\n",
    "    text = \" \".join(sent)\n",
    "\n",
    "    ## replace some general text alterations which would otherwise go undetected\n",
    "    text = text.replace(r\"&amp;\", \"and\")\n",
    "\n",
    "    text = text.replace(r\"\\n\", \"\")\n",
    "    \n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    ## expanding on contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    ## remove email IDs if present\n",
    "    text = re.sub(\"[\\w]+@[\\w]+\\.[c][o][m]\", \"[EMAIL]\", text)\n",
    "\n",
    "    ## replacing usernames in the text with generic keyword '@USERNAME'\n",
    "    #text=remove_username_mentions(text)\n",
    "\n",
    "\n",
    "    # removing unicode emojis from the text\n",
    "    # at the end for removing any trailing emoji patterns which add no value to the text\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # remove extra spacing from text\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = re.escape(string.punctuation)\n",
    "\n",
    "X_labelled = X_labelled.apply(lambda x: re.sub('['+chars+']', '',x))\n",
    "X_labelled = X_labelled.apply(lambda x: re.sub('[!@#$%^&*()[]{};:,./<>?\\|`~-=_+]', '',x))\n",
    "# since it did not undergo preprocessing\n",
    "X_labelled = X_labelled.apply(lambda x: usual_preprocess(x))\n",
    "\n",
    "X_unlabelled = X_unlabelled.apply(lambda x: re.sub('['+chars+']', '',x))\n",
    "X_unlabelled = X_unlabelled.apply(lambda x: re.sub('[!@#$%^&*()[]{};:,./<>?\\|`~-=_+]', '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le = preprocessing.LabelEncoder()\n",
    "#model = RandomForestClassifier()\n",
    "model = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression(solver='lbfgs', n_jobs=-1))\n",
    "])\n",
    "batch_size = 1000\n",
    "sampler = ConfidenceSampler(model, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_labelled = le.fit_transform(X_labelled)\n",
    "#X_unlabelled = le.fit_transform(X_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     She has NOTHING on Braverman Nothing Suella Br...\n",
       "1     10DowningStreet SuellaBraverman ukhomeoffice W...\n",
       "2     TheGMan benandjerrysUK SuellaBraverman Good on...\n",
       "3     Jerrygrey2002 drmikechao TsarKastik But I have...\n",
       "4     10DowningStreet SuellaBraverman ukhomeoffice O...\n",
       "                            ...                        \n",
       "95    Boris Johnson visited Gigg Lane stadium today ...\n",
       "96    EmilyThornberry PETAUK This at labour will not...\n",
       "97                     Queen :crown: AngelaRayner [URL]\n",
       "98    UK Prime Minister Boris Johnson lashed out at ...\n",
       "99    ladyhaja AngelaRayner Most debating I ever saw...\n",
       "Name: text, Length: 800, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(X_labelled, y_labelled, test_size=0.2, random_state=123)\n",
    "model.fit(X_train, y_train)  \n",
    "sampler.fit(X_train, y_train)\n",
    "selected = sampler.select_samples(X_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "# Evaluating performance\n",
    "accuracies.append(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.89375]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25173, 101800,   5837,  95577,  59925, 105604,  67677, 112937,\n",
       "        29456,   5663,  22593,  10645,  81087,  93567, 108467,  85316,\n",
       "        88695,  50284,  31760,  84580,  90951,   7201,  78613,  47548,\n",
       "        71565,  13296, 104244,  62675,  69997,  82125, 112736,  61296,\n",
       "        43257,  43613, 113940, 110423,  57301,  49457,  14645,  46241,\n",
       "         1449,  99014,  40718,  54199,  15388,  11668,  16811,  17315,\n",
       "        27196,  93662,  40944,  95815,  73984,   7820, 106798, 113485,\n",
       "        33141,  99258,  95517,  29602,  72827,  64147,  34355,   8541,\n",
       "        17280, 106643,  33499,   1618, 109684,  54184,  68281,   1023,\n",
       "        96508,  33262,  80923,  42634,  95299,  73767,  87208,   9235,\n",
       "        69238,  67986,  94967, 114678,  62336,  45968,  75881,  77180,\n",
       "        47145,  33102,  28616,  58547,  58151,  39632,   1223,  79210,\n",
       "        86840,  85897,  80249,  47304,  27312,  75449,  60795,  13952,\n",
       "        50495,  94608,  78563,  53498,  37224, 113372, 113476,  34709,\n",
       "       112776,  21258,  35020,  61961,  65261,  46589,  37363,  59823,\n",
       "        38374,  28064,  62793,  69841, 106465,  15910,  41000,  97487,\n",
       "        25319,  96779,  14623,  71075, 102769,  27640,  93477,  40790,\n",
       "        30231,  33344, 100472,  91666, 111505,  76016,  73963,  65161,\n",
       "        90555, 103464,  87444,  28407,  22693,    311, 100911,  59974,\n",
       "         3054, 112138,  39797, 110956,  82464,  42327,  20163,  45409,\n",
       "        43626,  68532,  17775,  57900,  51038,  92398,  63593,  84589,\n",
       "        82819, 112133,  80909,   7940, 114877, 113224,  90514,  41388,\n",
       "        47877,  47060,   2122,  94162,  24101,   3122,  62187,  21515,\n",
       "        23297,   5152,  24017,  74643,  26317,  24868,  59895,  95291,\n",
       "       113244,  57683,  19615,  96348,  32048,  43487,  38933, 103117,\n",
       "        51860,  29039,  69682,  85264,  58309,  99900,   4355,  81098,\n",
       "        54565,  11354,  54634,  42111,  26472,  56301, 105205,  49435,\n",
       "        97333,  51367,  20927,  79177,  10897,  21466,  91114,  12846,\n",
       "        79339,   4166,  27739, 101568,  47338,  88910,  65459,  49297,\n",
       "        80185,  80977,   5976,  47513,  21278,  83309, 111284,  90335,\n",
       "        89045,  23390,  17542,  70744, 110051,  19319,  90522,  58490,\n",
       "        33853,  39129,  37465,  20519, 114612,  28819,  43436,  86381,\n",
       "        83889,  17198,  43515,  24527, 101765,  25755,  68222,   2370,\n",
       "       107631,   3627,  92698,  22253,  26716, 111920, 113205,  15307,\n",
       "        78502, 101055,  68635,  82765, 101441,  54955,  82861,  58935,\n",
       "       101337,  23254,  83630,  96718,  75703, 104212,  43450,  88163,\n",
       "        44097,  44594,  67630,  53092,  19616,  63894,  78508,  69338,\n",
       "        97144,  96833,  86922,  52849,  21176,  77931,  57737,  90005,\n",
       "        47592,  95178,  99585,   8663,   1239,  96395,  64561,    495,\n",
       "        28849,  74677,  44452,  41878,  54608,  69331,  45090, 113266,\n",
       "        22106,  49283,  95700, 110307,  19557,   4640,  90742,  70699,\n",
       "        31164,  41435,  36003,  48066,  76262,  11048,  36752,  73846,\n",
       "        64667,  65125,  61223,  59420,  44934,  55635,  15858,  49828,\n",
       "         6157,  43074,  50183,  34242, 108748,  24726,  51826,  32974,\n",
       "        34403,  19738, 108703,  73358, 111538,  64919,  76816,  32873,\n",
       "        84648,  30752,   2715,   9850, 109739,  22702,  51661,  71392,\n",
       "        95032,  79941,  41463,  86568, 107954,  76393,  87812, 112190,\n",
       "        93399,  90102, 112354,  42778, 113484,   1961,  49539,  67297,\n",
       "        99135, 113883,  66020,    975,  28024, 110840,  43440,  50633,\n",
       "        76731,  53749,  58384,  15315, 111054,  45312, 101211,  66160,\n",
       "        29619,  38356,  31572,  82376,  94272,  75142,  97479,  29681,\n",
       "        43456,  65377,  93734,  13677,  51121,  26616,  80534,  22697,\n",
       "        42623,  58858,   8414,  82121,  70169,  87051, 104764,  26715,\n",
       "         3903,  83103,  25880, 106487,  95990,  36275,  37481,   1272,\n",
       "        79381, 113215,  18187,   1268,  94734,  34265,  42975,  80949,\n",
       "        61848,  71864, 111650,  25812,  70171,  14066,   5595, 113240,\n",
       "       100289, 114810,  41158,  49861,  15092,  63314,  32789,  29365,\n",
       "       110937,  39856,  10876,  37164,  48286,  95716,  98357,   2290,\n",
       "       114926,  17116, 107651, 112410,  45114,  18616,  44671,  97429,\n",
       "        39620,  98444, 111750,  63568,   2521, 110387,  40129,  75219,\n",
       "        79438,  70261,  56321, 103072,  12411,  47074,  81695,  87898,\n",
       "       109024,  52764,  49365,  84924,   6632,  33021, 114481,  70110,\n",
       "        72072,  61826,  73302,  49561,  79357,  34126,  90088,  42120,\n",
       "        97282,  26098,  82991,  51233,  70751, 114583,  85707,  73870,\n",
       "        83105,  57086,  57503, 111747, 110257,  29633, 112511,  63928,\n",
       "         6990,  47915,  65369,  69481,  89324,  75583,  34998,  32328,\n",
       "        92733,  45703,   7378,  22679,  40682,  49805,  13550,  11735,\n",
       "        26268,  50787,  75939,  17493,  84874,  13297,   2636,  62571,\n",
       "        84612,  90933,  68386, 110534,    761, 102251,   4460,  47068,\n",
       "       112927,  68323,  13084,  76792,  24211,  48892,  42284,  67619,\n",
       "         2887,  98087,  32117,  71070,  44266,  32573,  47833, 103184,\n",
       "        67409, 108416,  13220,  66263, 100651,  45107,  27182,  73927,\n",
       "        88367,  33847,  82226,   2269,  73176,  95244,   8506,   5924,\n",
       "        86921,  90542,  76635,  67676,  62282, 101327,  77327,  91293,\n",
       "        40329,   6197,  26904,  99226,   2254,  78084,  11011,  55822,\n",
       "       104739, 112434,  79593,  60854,  74193,  76022, 112533,  59585,\n",
       "        64378,  46768,   5699,  68106,  11922,  31617,  38710,  38087,\n",
       "        88223,  22958,  89503,  50650, 103992,  77053,   1556,   5809,\n",
       "        36996,  38309,  50389,   3692, 110675,  12102,   3434,  99391,\n",
       "        66407,  28604,  95242,  29129,  84899,  36702,  53091,  32961,\n",
       "        19951,  22441,  13794,  78952,  84743,  89236,  36054,  90313,\n",
       "        59431,   1807,  61244,  58298,  90128,  28403,  25917, 107767,\n",
       "        64784,  95890,  35503,  94504,  91900,  41536,  73085, 113363,\n",
       "        64478,  13172, 110727, 106654,  37106, 109083,  72087, 112233,\n",
       "        15051, 113560,  72458, 109171,  32366,  76419, 114776,  12780,\n",
       "        50431,  41385, 114389,  93732,  95632,  28730,  88852,  89433,\n",
       "        36091,  34257,  27145,  34801,  23439,  22859, 114066,  85474,\n",
       "         7904,  57955,  89273, 100931,  95222, 114456,  94784, 111225,\n",
       "        44202,  14680,  75053, 112040,  76765,  95622,  93178,  22928,\n",
       "        74098,   3326,  43501,  87490,  46758,   2810,  66005,  82413,\n",
       "        21231,  64604,  22453,  42962,  64984, 104609,  12277,  73554,\n",
       "        87516,  27542,  31876,    935, 107913,  52061,  26963,  24182,\n",
       "        89099,  23772,   9399,  66676,  90391,  50673,  90594,   4697,\n",
       "        21734,  61832,  89290,  16453, 105020, 102527,  28367, 111292,\n",
       "        74338,  11802,  57514,  87631,  78968,    447,  12813,  91034,\n",
       "       102732,  22838,  98006,  33564,   5236,  18194,  92719,  90318,\n",
       "       112160,  48722,  43008,  36154,  44224,  17597,  79578, 109516,\n",
       "        69098,  60220,  41332,   5252,  15441,  93778,  49281,  77394,\n",
       "        54286,  11362,  11899,  57907,  86535,  42865,  95386,  83262,\n",
       "        29776,  23676,  27305,  80265,  56415, 101371,  57926,  95454,\n",
       "        27281,  91256,  71080,  40125,  65996,  28764, 107093,  10327,\n",
       "        41900,  53095,  79875,  60647,   4387,  61673, 113982,  12523,\n",
       "       112036,  21104,  48712,  55362,  20730,  68465,  51668,  84865,\n",
       "        33685,  90408,  94622,  55468,  78685, 104232,  33293, 101384,\n",
       "        41687,  52339,  59135, 112173,  98428,  60241,  15390,  54198,\n",
       "        99737,  52552,  22817,  93842,  38288,  82586,  88130,  35671,\n",
       "        23654,  88832,  33055,  47880,  90596,  33267,  40127,  49518,\n",
       "        55691,  31364,  47924,  15016, 104207,  44899,  19287,  68133,\n",
       "        75167,  35118,  69962,  38515,  17346,  49095,  22348,  87879,\n",
       "        10591,  41633,  74954,  56037,   1120,  68806,  82235, 113614,\n",
       "        34581,  80002,  40309, 111219,  66648,  48647,  98483,  96877,\n",
       "        13212,  87627,  42946,  61309,  41251,  96282,  59168,   4499,\n",
       "         2338,  79243,  83579,   2067,  13349,  56480,   8933,  62285,\n",
       "       112794,  34888,  91208,  70829,  68948,  73526,  36286, 114419,\n",
       "        89031, 111987,  15360,  73042,  48257,  67021, 101691,    959,\n",
       "        92266, 113799,  68436,  93476,  57612,  42379,  11657,  45422,\n",
       "        76220,  52536,  90443,  79471,  29179,  91899,   7505, 114927,\n",
       "        22357,  81336,   1313,  56024,  87593,   6040,  20379,  90494,\n",
       "       109613,  70168,  47556,  39190,  20293,  69293, 100302,  51127,\n",
       "        44796,  46722,  71107,  61902,  74268,  30251,  56766,  98226,\n",
       "        36535,  96353,  80411, 110286,  50827,  32174,  33320,  63872,\n",
       "        22729,  29892,  27034, 100361,  95114,  99617,  14000,  28591,\n",
       "        40339,    570,  16790,  31708, 103996,  46465,  22054, 113606,\n",
       "        10752,  18879,  23456,  47808,  54140,  40352,  46259, 113119,\n",
       "        63969,  85262,  30590,  97743,  11593,  22996,  86498,  39656])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the labeled and unlabeled pool\n",
    "'''X_labelled = np.concatenate([X_labelled, selected])\n",
    "# The selected samples are sent to be labeled as y_selected\n",
    "y_labelled = np.concatenate([y_labelled, y_selected])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will not use the above method as it gives random IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring hard to classify examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_labelled_dataset(num_samples_per_class=5):\n",
    "  \"\"\"\n",
    "  Getting the initial balanced dataset. Although this is random in this code.\n",
    "  In actual production system it should be informative labelled dataset given by the annotators/SMEs.\n",
    "\n",
    "  Args:\n",
    "    num_samples_per_class: int, number of samples to be takes per class in the first iteration.\n",
    "\n",
    "  Returns:\n",
    "    numpy array of labelled dataset with true labels and pooled dataset with true labels (for performance checking)\n",
    "  \"\"\"\n",
    "\n",
    "  X = list()\n",
    "  y = list()\n",
    "  X_pooled = list()\n",
    "  y_pooled = list()\n",
    "  labelled_idx = list()\n",
    "\n",
    "  counter_dict = dict()\n",
    "\n",
    "  for idx, target in enumerate(digits['target']):\n",
    "    if target in counter_dict:\n",
    "      if counter_dict[target] == num_samples_per_class:\n",
    "        continue\n",
    "      counter_dict[target] += 1\n",
    "    else:\n",
    "      counter_dict[target] = 1\n",
    "    X.append(digits['data'][idx])\n",
    "    y.append(target)\n",
    "    labelled_idx.append(idx)\n",
    "\n",
    "  X_pooled = np.delete(digits['data'], labelled_idx, axis=0)\n",
    "  y_pooled = np.delete(digits['target'], labelled_idx)\n",
    "\n",
    "  return np.asarray(X), np.asarray(y), X_pooled, y_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antisexism_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
